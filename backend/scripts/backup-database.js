#!/usr/bin/env node

/**
 * Smart Tourist Safety System - Database Backup Script
 * Automated database backup with encryption, compression, and multiple targets
 * 
 * ðŸ—„ï¸ DATABASE BACKUP - Complete PostgreSQL/Supabase backup
 * ðŸ” ENCRYPTION - AES-256 encrypted backup files
 * ðŸ“¦ COMPRESSION - GZIP compression for storage efficiency
 * â˜ï¸ MULTI-TARGET - Local, cloud, and remote backup destinations
 * ðŸ”„ INCREMENTAL - Support for full and incremental backups
 * âš¡ PARALLEL - Multi-threaded backup operations
 */

const fs = require('fs').promises;
const path = require('path');
const { spawn, exec } = require('child_process');
const crypto = require('crypto');
const zlib = require('zlib');
const { pipeline } = require('stream/promises');
const util = require('util');

// Convert exec to promise
const execAsync = util.promisify(exec);

// ============================================================================
// CONFIGURATION
// ============================================================================

const config = {
  // Database configuration
  database: {
    host: process.env.DB_HOST || 'localhost',
    port: process.env.DB_PORT || 5432,
    database: process.env.DB_NAME || 'tourist_safety',
    username: process.env.DB_USER || 'postgres',
    password: process.env.DB_PASSWORD || '',
    supabaseUrl: process.env.SUPABASE_URL || '',
    supabaseServiceKey: process.env.SUPABASE_SERVICE_ROLE_KEY || ''
  },
  
  // Backup configuration
  backup: {
    baseDir: process.env.BACKUP_DIR || './backups',
    retentionDays: parseInt(process.env.BACKUP_RETENTION_DAYS) || 30,
    compressionLevel: parseInt(process.env.COMPRESSION_LEVEL) || 6,
    encryptionKey: process.env.BACKUP_ENCRYPTION_KEY || '',
    parallel: parseInt(process.env.BACKUP_PARALLEL) || 4
  },
  
  // Storage destinations
  destinations: [
    {
      type: 'local',
      path: './backups/local',
      enabled: true,
      priority: 1
    },
    {
      type: 'aws-s3',
      bucket: process.env.AWS_S3_BACKUP_BUCKET || '',
      region: process.env.AWS_REGION || 'us-east-1',
      enabled: !!process.env.AWS_S3_BACKUP_BUCKET,
      priority: 2
    },
    {
      type: 'azure-blob',
      account: process.env.AZURE_STORAGE_ACCOUNT || '',
      container: process.env.AZURE_BACKUP_CONTAINER || '',
      enabled: !!process.env.AZURE_STORAGE_ACCOUNT,
      priority: 3
    }
  ]
};

// ============================================================================
// BACKUP MANAGER CLASS
// ============================================================================

class DatabaseBackupManager {
  constructor() {
    this.startTime = new Date();
    this.backupId = this.generateBackupId();
    this.tempDir = path.join(config.backup.baseDir, 'temp', this.backupId);
    this.logFile = path.join(config.backup.baseDir, 'logs', `backup_${this.backupId}.log`);
    this.metadata = {
      id: this.backupId,
      timestamp: this.startTime.toISOString(),
      type: 'unknown',
      status: 'started',
      size: 0,
      checksum: '',
      tables: [],
      errors: []
    };
  }

  // ==========================================================================
  // INITIALIZATION
  // ==========================================================================

  async initialize() {
    console.log(`[BackupManager] Initializing backup ${this.backupId}...`);
    
    // Create necessary directories
    await fs.mkdir(config.backup.baseDir, { recursive: true });
    await fs.mkdir(path.join(config.backup.baseDir, 'full'), { recursive: true });
    await fs.mkdir(path.join(config.backup.baseDir, 'incremental'), { recursive: true });
    await fs.mkdir(path.join(config.backup.baseDir, 'logs'), { recursive: true });
    await fs.mkdir(this.tempDir, { recursive: true });
    
    // Initialize log file
    await this.log('Backup process started');
    
    // Check dependencies
    await this.checkDependencies();
    
    console.log(`[BackupManager] Backup ${this.backupId} initialized successfully`);
  }

  async checkDependencies() {
    try {
      // Check pg_dump availability
      await execAsync('pg_dump --version');
      await this.log('pg_dump is available');
    } catch (error) {
      throw new Error('pg_dump is not available. Please install PostgreSQL client tools.');
    }

    // Check database connectivity
    try {
      const connectionString = this.buildConnectionString();
      await execAsync(`pg_isready -d "${connectionString}"`);
      await this.log('Database connection verified');
    } catch (error) {
      await this.log(`Database connection failed: ${error.message}`, 'error');
      throw new Error('Cannot connect to database');
    }
  }

  // ==========================================================================
  // BACKUP OPERATIONS
  // ==========================================================================

  async performFullBackup() {
    console.log(`[BackupManager] Starting full database backup...`);
    this.metadata.type = 'full';
    
    try {
      await this.log('Starting full backup process');
      
      // Step 1: Get database schema and tables
      const tables = await this.getDatabaseTables();
      this.metadata.tables = tables;
      await this.log(`Found ${tables.length} tables to backup`);
      
      // Step 2: Create schema dump
      await this.backupSchema();
      
      // Step 3: Backup data (parallel for large tables)
      await this.backupData(tables);
      
      // Step 4: Backup sequences and functions
      await this.backupSequencesAndFunctions();
      
      // Step 5: Create consolidated backup file
      const backupFile = await this.consolidateBackup();
      
      // Step 6: Compress and encrypt
      const finalFile = await this.compressAndEncrypt(backupFile);
      
      // Step 7: Generate metadata
      await this.generateMetadata(finalFile);
      
      // Step 8: Upload to destinations
      await this.uploadToDestinations(finalFile);
      
      // Step 9: Cleanup
      await this.cleanup();
      
      this.metadata.status = 'completed';
      console.log(`[BackupManager] Full backup completed successfully: ${finalFile}`);
      
      return finalFile;
      
    } catch (error) {
      this.metadata.status = 'failed';
      this.metadata.errors.push(error.message);
      await this.log(`Full backup failed: ${error.message}`, 'error');
      throw error;
    }
  }

  async performIncrementalBackup() {
    console.log(`[BackupManager] Starting incremental database backup...`);
    this.metadata.type = 'incremental';
    
    try {
      await this.log('Starting incremental backup process');
      
      // Get last backup timestamp
      const lastBackupTime = await this.getLastBackupTimestamp();
      await this.log(`Last backup timestamp: ${lastBackupTime}`);
      
      // Get tables with changes
      const changedTables = await this.getChangedTables(lastBackupTime);
      this.metadata.tables = changedTables;
      
      if (changedTables.length === 0) {
        await this.log('No changes detected since last backup');
        this.metadata.status = 'completed';
        return null;
      }
      
      await this.log(`Found changes in ${changedTables.length} tables`);
      
      // Backup only changed data
      await this.backupIncrementalData(changedTables, lastBackupTime);
      
      // Create backup file
      const backupFile = await this.consolidateBackup();
      
      // Compress and encrypt
      const finalFile = await this.compressAndEncrypt(backupFile);
      
      // Generate metadata
      await this.generateMetadata(finalFile);
      
      // Upload to destinations
      await this.uploadToDestinations(finalFile);
      
      // Cleanup
      await this.cleanup();
      
      this.metadata.status = 'completed';
      console.log(`[BackupManager] Incremental backup completed: ${finalFile}`);
      
      return finalFile;
      
    } catch (error) {
      this.metadata.status = 'failed';
      this.metadata.errors.push(error.message);
      await this.log(`Incremental backup failed: ${error.message}`, 'error');
      throw error;
    }
  }

  // ==========================================================================
  // DATABASE OPERATIONS
  // ==========================================================================

  async getDatabaseTables() {
    const query = `
      SELECT table_name 
      FROM information_schema.tables 
      WHERE table_schema = 'public' 
      AND table_type = 'BASE TABLE'
      ORDER BY table_name;
    `;
    
    const result = await this.executeQuery(query);
    return result.stdout.trim().split('\n').filter(table => table.trim());
  }

  async backupSchema() {
    await this.log('Backing up database schema...');
    
    const schemaFile = path.join(this.tempDir, 'schema.sql');
    const connectionString = this.buildConnectionString();
    
    const command = `pg_dump "${connectionString}" --schema-only --no-owner --no-privileges -f "${schemaFile}"`;
    
    try {
      await execAsync(command);
      await this.log('Schema backup completed');
    } catch (error) {
      throw new Error(`Schema backup failed: ${error.message}`);
    }
  }

  async backupData(tables) {
    await this.log('Backing up table data...');
    
    // Split tables into chunks for parallel processing
    const chunks = this.chunkArray(tables, config.backup.parallel);
    
    for (let i = 0; i < chunks.length; i++) {
      const chunk = chunks[i];
      await this.log(`Processing chunk ${i + 1}/${chunks.length} with ${chunk.length} tables`);
      
      // Process chunk in parallel
      const promises = chunk.map(table => this.backupTable(table));
      await Promise.all(promises);
    }
    
    await this.log('All table data backed up successfully');
  }

  async backupTable(tableName) {
    const tableFile = path.join(this.tempDir, `${tableName}.sql`);
    const connectionString = this.buildConnectionString();
    
    const command = `pg_dump "${connectionString}" --data-only --table="${tableName}" --no-owner --no-privileges -f "${tableFile}"`;
    
    try {
      await execAsync(command);
      await this.log(`Table ${tableName} backed up successfully`);
    } catch (error) {
      await this.log(`Failed to backup table ${tableName}: ${error.message}`, 'error');
      throw error;
    }
  }

  async backupSequencesAndFunctions() {
    await this.log('Backing up sequences and functions...');
    
    const sequencesFile = path.join(this.tempDir, 'sequences.sql');
    const functionsFile = path.join(this.tempDir, 'functions.sql');
    const connectionString = this.buildConnectionString();
    
    try {
      // Backup sequences
      const sequencesCommand = `pg_dump "${connectionString}" --schema-only --no-owner --no-privileges -t '*_seq' -f "${sequencesFile}"`;
      await execAsync(sequencesCommand);
      
      // Backup functions and procedures
      const functionsCommand = `pg_dump "${connectionString}" --schema-only --no-owner --no-privileges -P -f "${functionsFile}"`;
      await execAsync(functionsCommand);
      
      await this.log('Sequences and functions backed up successfully');
    } catch (error) {
      await this.log(`Failed to backup sequences/functions: ${error.message}`, 'error');
      throw error;
    }
  }

  async backupIncrementalData(tables, sinceTimestamp) {
    await this.log(`Backing up incremental data since ${sinceTimestamp}...`);
    
    for (const tableName of tables) {
      try {
        await this.backupTableIncremental(tableName, sinceTimestamp);
      } catch (error) {
        await this.log(`Failed to backup incremental data for ${tableName}: ${error.message}`, 'error');
        throw error;
      }
    }
  }

  async backupTableIncremental(tableName, sinceTimestamp) {
    const tableFile = path.join(this.tempDir, `${tableName}_incremental.sql`);
    const connectionString = this.buildConnectionString();
    
    // Try to find timestamp column (common names)
    const timestampColumns = ['updated_at', 'modified_at', 'created_at', 'timestamp'];
    let timestampColumn = null;
    
    for (const col of timestampColumns) {
      try {
        const checkQuery = `SELECT 1 FROM information_schema.columns WHERE table_name = '${tableName}' AND column_name = '${col}' LIMIT 1;`;
        const result = await this.executeQuery(checkQuery);
        if (result.stdout.trim()) {
          timestampColumn = col;
          break;
        }
      } catch (error) {
        // Column doesn't exist, continue
      }
    }
    
    if (!timestampColumn) {
      await this.log(`No timestamp column found for ${tableName}, performing full backup`);
      await this.backupTable(tableName);
      return;
    }
    
    // Create incremental backup with WHERE clause
    const whereClause = `WHERE ${timestampColumn} > '${sinceTimestamp}'`;
    const command = `pg_dump "${connectionString}" --data-only --table="${tableName}" --where="${whereClause}" --no-owner --no-privileges -f "${tableFile}"`;
    
    await execAsync(command);
    await this.log(`Incremental backup for ${tableName} completed`);
  }

  async getChangedTables(sinceTimestamp) {
    // This is a simplified version - in production, you might want to:
    // 1. Use database change tracking features
    // 2. Check table modification times
    // 3. Use triggers to track changes
    
    const tables = await this.getDatabaseTables();
    const changedTables = [];
    
    for (const table of tables) {
      try {
        const hasChanges = await this.tableHasChanges(table, sinceTimestamp);
        if (hasChanges) {
          changedTables.push(table);
        }
      } catch (error) {
        await this.log(`Error checking changes for ${table}: ${error.message}`, 'warn');
        // Include table in backup to be safe
        changedTables.push(table);
      }
    }
    
    return changedTables;
  }

  async tableHasChanges(tableName, sinceTimestamp) {
    const timestampColumns = ['updated_at', 'modified_at', 'created_at', 'timestamp'];
    
    for (const col of timestampColumns) {
      try {
        const query = `SELECT COUNT(*) FROM ${tableName} WHERE ${col} > '${sinceTimestamp}' LIMIT 1;`;
        const result = await this.executeQuery(query);
        const count = parseInt(result.stdout.trim());
        if (count > 0) {
          return true;
        }
      } catch (error) {
        // Column might not exist, continue with next
        continue;
      }
    }
    
    return false; // No timestamp column found or no recent changes
  }

  // ==========================================================================
  // FILE OPERATIONS
  // ==========================================================================

  async consolidateBackup() {
    await this.log('Consolidating backup files...');
    
    const consolidatedFile = path.join(this.tempDir, 'consolidated_backup.sql');
    const writeStream = require('fs').createWriteStream(consolidatedFile);
    
    try {
      // Header with metadata
      writeStream.write(`-- Smart Tourist Safety System Database Backup\n`);
      writeStream.write(`-- Backup ID: ${this.backupId}\n`);
      writeStream.write(`-- Timestamp: ${this.metadata.timestamp}\n`);
      writeStream.write(`-- Type: ${this.metadata.type}\n`);
      writeStream.write(`-- Tables: ${this.metadata.tables.join(', ')}\n\n`);
      
      // Add schema first (for full backups)
      if (this.metadata.type === 'full') {
        const schemaFile = path.join(this.tempDir, 'schema.sql');
        try {
          const schemaContent = await fs.readFile(schemaFile, 'utf8');
          writeStream.write('-- SCHEMA\n');
          writeStream.write(schemaContent);
          writeStream.write('\n\n');
        } catch (error) {
          await this.log(`Warning: Could not include schema: ${error.message}`, 'warn');
        }
      }
      
      // Add data files
      const files = await fs.readdir(this.tempDir);
      const dataFiles = files.filter(file => file.endsWith('.sql') && file !== 'schema.sql' && file !== 'consolidated_backup.sql');
      
      for (const file of dataFiles) {
        const filePath = path.join(this.tempDir, file);
        const content = await fs.readFile(filePath, 'utf8');
        
        writeStream.write(`-- DATA FROM: ${file}\n`);
        writeStream.write(content);
        writeStream.write('\n\n');
      }
      
      // Add sequences and functions (for full backups)
      if (this.metadata.type === 'full') {
        const sequencesFile = path.join(this.tempDir, 'sequences.sql');
        const functionsFile = path.join(this.tempDir, 'functions.sql');
        
        try {
          const sequencesContent = await fs.readFile(sequencesFile, 'utf8');
          writeStream.write('-- SEQUENCES\n');
          writeStream.write(sequencesContent);
          writeStream.write('\n\n');
        } catch (error) {
          await this.log(`Warning: Could not include sequences: ${error.message}`, 'warn');
        }
        
        try {
          const functionsContent = await fs.readFile(functionsFile, 'utf8');
          writeStream.write('-- FUNCTIONS\n');
          writeStream.write(functionsContent);
          writeStream.write('\n\n');
        } catch (error) {
          await this.log(`Warning: Could not include functions: ${error.message}`, 'warn');
        }
      }
      
      writeStream.end();
      
      await new Promise((resolve, reject) => {
        writeStream.on('finish', resolve);
        writeStream.on('error', reject);
      });
      
      await this.log('Backup consolidation completed');
      return consolidatedFile;
      
    } catch (error) {
      throw new Error(`Failed to consolidate backup: ${error.message}`);
    }
  }

  async compressAndEncrypt(sourceFile) {
    await this.log('Compressing and encrypting backup...');
    
    const timestamp = this.startTime.toISOString().replace(/[:.]/g, '-');
    const finalFileName = `${this.metadata.type}_backup_${this.backupId}_${timestamp}.sql.gz.enc`;
    const finalFile = path.join(config.backup.baseDir, this.metadata.type, finalFileName);
    
    // Ensure target directory exists
    await fs.mkdir(path.dirname(finalFile), { recursive: true });
    
    try {
      // Step 1: Compress
      const compressedFile = `${sourceFile}.gz`;
      await this.compressFile(sourceFile, compressedFile);
      
      // Step 2: Encrypt
      if (config.backup.encryptionKey) {
        await this.encryptFile(compressedFile, finalFile);
        // Clean up compressed file
        await fs.unlink(compressedFile);
      } else {
        // No encryption, just move compressed file
        await fs.rename(compressedFile, finalFile);
        await this.log('Warning: No encryption key provided, backup is not encrypted', 'warn');
      }
      
      await this.log('Compression and encryption completed');
      return finalFile;
      
    } catch (error) {
      throw new Error(`Failed to compress/encrypt backup: ${error.message}`);
    }
  }

  async compressFile(sourceFile, targetFile) {
    const readStream = require('fs').createReadStream(sourceFile);
    const writeStream = require('fs').createWriteStream(targetFile);
    const gzipStream = zlib.createGzip({ level: config.backup.compressionLevel });
    
    await pipeline(readStream, gzipStream, writeStream);
  }

  async encryptFile(sourceFile, targetFile) {
    const algorithm = 'aes-256-cbc';
    const key = crypto.scryptSync(config.backup.encryptionKey, 'salt', 32);
    const iv = crypto.randomBytes(16);
    
    const cipher = crypto.createCipher(algorithm, key);
    
    const readStream = require('fs').createReadStream(sourceFile);
    const writeStream = require('fs').createWriteStream(targetFile);
    
    // Write IV to the beginning of the file
    writeStream.write(iv);
    
    await pipeline(readStream, cipher, writeStream);
  }

  // ==========================================================================
  // METADATA AND CLEANUP
  // ==========================================================================

  async generateMetadata(backupFile) {
    await this.log('Generating backup metadata...');
    
    const stats = await fs.stat(backupFile);
    this.metadata.size = stats.size;
    this.metadata.checksum = await this.calculateChecksum(backupFile);
    this.metadata.destination = backupFile;
    this.metadata.duration = Date.now() - this.startTime.getTime();
    
    // Save metadata
    const metadataFile = path.join(config.backup.baseDir, 'metadata.json');
    let existingMetadata = [];
    
    try {
      const data = await fs.readFile(metadataFile, 'utf8');
      existingMetadata = JSON.parse(data);
    } catch (error) {
      // File doesn't exist, start with empty array
    }
    
    existingMetadata.push(this.metadata);
    await fs.writeFile(metadataFile, JSON.stringify(existingMetadata, null, 2));
    
    await this.log('Metadata generation completed');
  }

  async calculateChecksum(filePath) {
    const hash = crypto.createHash('sha256');
    const stream = require('fs').createReadStream(filePath);
    
    return new Promise((resolve, reject) => {
      stream.on('data', (data) => hash.update(data));
      stream.on('end', () => resolve(hash.digest('hex')));
      stream.on('error', reject);
    });
  }

  async cleanup() {
    await this.log('Cleaning up temporary files...');
    
    try {
      await fs.rmdir(this.tempDir, { recursive: true });
      await this.log('Temporary files cleaned up');
    } catch (error) {
      await this.log(`Warning: Could not clean up temp directory: ${error.message}`, 'warn');
    }
    
    // Clean up old backups based on retention policy
    await this.cleanupOldBackups();
  }

  async cleanupOldBackups() {
    await this.log('Cleaning up old backups...');
    
    const retentionMs = config.backup.retentionDays * 24 * 60 * 60 * 1000;
    const cutoffDate = new Date(Date.now() - retentionMs);
    
    const metadataFile = path.join(config.backup.baseDir, 'metadata.json');
    
    try {
      const data = await fs.readFile(metadataFile, 'utf8');
      const metadata = JSON.parse(data);
      
      const toDelete = metadata.filter(item => new Date(item.timestamp) < cutoffDate);
      const toKeep = metadata.filter(item => new Date(item.timestamp) >= cutoffDate);
      
      for (const item of toDelete) {
        try {
          await fs.unlink(item.destination);
          await this.log(`Deleted old backup: ${item.id}`);
        } catch (error) {
          await this.log(`Could not delete old backup ${item.id}: ${error.message}`, 'warn');
        }
      }
      
      // Update metadata file
      await fs.writeFile(metadataFile, JSON.stringify(toKeep, null, 2));
      
      await this.log(`Cleaned up ${toDelete.length} old backups`);
    } catch (error) {
      await this.log(`Could not clean up old backups: ${error.message}`, 'warn');
    }
  }

  // ==========================================================================
  // UPLOAD OPERATIONS
  // ==========================================================================

  async uploadToDestinations(backupFile) {
    await this.log('Uploading backup to destinations...');
    
    const enabledDestinations = config.destinations.filter(dest => dest.enabled);
    
    for (const destination of enabledDestinations) {
      try {
        await this.uploadToDestination(backupFile, destination);
        await this.log(`Uploaded to ${destination.type} successfully`);
      } catch (error) {
        await this.log(`Failed to upload to ${destination.type}: ${error.message}`, 'error');
        // Continue with other destinations
      }
    }
  }

  async uploadToDestination(backupFile, destination) {
    const fileName = path.basename(backupFile);
    
    switch (destination.type) {
      case 'local':
        await fs.mkdir(destination.path, { recursive: true });
        const localTarget = path.join(destination.path, fileName);
        await fs.copyFile(backupFile, localTarget);
        break;
        
      case 'aws-s3':
        // Mock S3 upload - in production, use AWS SDK
        await this.log(`Mock S3 upload to bucket: ${destination.bucket}`);
        break;
        
      case 'azure-blob':
        // Mock Azure upload - in production, use Azure SDK
        await this.log(`Mock Azure Blob upload to account: ${destination.account}`);
        break;
        
      default:
        throw new Error(`Unsupported destination type: ${destination.type}`);
    }
  }

  // ==========================================================================
  // UTILITY METHODS
  // ==========================================================================

  generateBackupId() {
    const timestamp = Date.now();
    const random = Math.random().toString(36).substring(2, 8);
    return `backup_${timestamp}_${random}`;
  }

  buildConnectionString() {
    if (config.database.supabaseUrl && config.database.supabaseServiceKey) {
      // Supabase connection
      const url = new URL(config.database.supabaseUrl);
      return `postgresql://postgres:${config.database.supabaseServiceKey}@${url.hostname}:5432/postgres`;
    } else {
      // Standard PostgreSQL connection
      return `postgresql://${config.database.username}:${config.database.password}@${config.database.host}:${config.database.port}/${config.database.database}`;
    }
  }

  async executeQuery(query) {
    const connectionString = this.buildConnectionString();
    const command = `psql "${connectionString}" -t -c "${query.replace(/"/g, '\\"')}"`;
    return await execAsync(command);
  }

  async getLastBackupTimestamp() {
    try {
      const metadataFile = path.join(config.backup.baseDir, 'metadata.json');
      const data = await fs.readFile(metadataFile, 'utf8');
      const metadata = JSON.parse(data);
      
      const lastBackup = metadata
        .filter(item => item.status === 'completed')
        .sort((a, b) => new Date(b.timestamp).getTime() - new Date(a.timestamp).getTime())[0];
      
      return lastBackup ? lastBackup.timestamp : new Date(0).toISOString();
    } catch (error) {
      return new Date(0).toISOString();
    }
  }

  chunkArray(array, chunkSize) {
    const chunks = [];
    for (let i = 0; i < array.length; i += chunkSize) {
      chunks.push(array.slice(i, i + chunkSize));
    }
    return chunks;
  }

  async log(message, level = 'info') {
    const timestamp = new Date().toISOString();
    const logEntry = `[${timestamp}] [${level.toUpperCase()}] ${message}\n`;
    
    console.log(`[BackupManager] ${message}`);
    
    try {
      await fs.appendFile(this.logFile, logEntry);
    } catch (error) {
      console.error(`Failed to write to log file: ${error.message}`);
    }
  }
}

// ============================================================================
// MAIN EXECUTION
// ============================================================================

async function main() {
  const args = process.argv.slice(2);
  const backupType = args[0] || 'full'; // 'full' or 'incremental'
  
  if (!['full', 'incremental'].includes(backupType)) {
    console.error('Usage: node backup-database.js [full|incremental]');
    process.exit(1);
  }
  
  const manager = new DatabaseBackupManager();
  
  try {
    await manager.initialize();
    
    let result;
    if (backupType === 'full') {
      result = await manager.performFullBackup();
    } else {
      result = await manager.performIncrementalBackup();
    }
    
    if (result) {
      console.log(`\nâœ… Backup completed successfully!`);
      console.log(`ðŸ“ Backup file: ${result}`);
      console.log(`ðŸ†” Backup ID: ${manager.backupId}`);
      console.log(`â±ï¸  Duration: ${Date.now() - manager.startTime.getTime()}ms`);
    } else {
      console.log(`\nðŸ“‹ No changes detected, backup skipped.`);
    }
    
    process.exit(0);
  } catch (error) {
    console.error(`\nâŒ Backup failed: ${error.message}`);
    process.exit(1);
  }
}

// Run if called directly
if (require.main === module) {
  main().catch(error => {
    console.error('Unhandled error:', error);
    process.exit(1);
  });
}

module.exports = { DatabaseBackupManager, config };